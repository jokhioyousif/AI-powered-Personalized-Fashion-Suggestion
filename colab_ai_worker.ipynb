{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# connect gdrive to access inswapper onxx model\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Connecting to Google Drive...\")\n",
        "# This will prompt you for authorization./\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9zavgVWg48SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# This is the path to the file in your Google Drive.\n",
        "# If you did not put it inside any folders, this path is correct.\n",
        "drive_file_path = '/content/drive/My Drive/inswapper_128.onnx'\n",
        "\n",
        "# This is the destination folder the AI script needs.\n",
        "target_dir = '/root/.insightface/models'\n",
        "target_path = os.path.join(target_dir, 'inswapper_128.onnx')\n",
        "\n",
        "print(f\"Attempting to copy file from your Drive: {drive_file_path}\")\n",
        "\n",
        "try:\n",
        "    # Create the destination directory\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Copy the file from Google Drive to the Colab environment\n",
        "    shutil.copy(drive_file_path, target_path)\n",
        "\n",
        "    print(f\"✅ Success! Model has been copied to {target_path}\")\n",
        "    print(\"\\nSetup is complete! You can now run the main Fashion AI script.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: The file was not found at '{drive_file_path}'.\")\n",
        "    print(\"Please check two things:\")\n",
        "    print(\"1. That the file is named 'inswapper_128.onnx' in your Drive.\")\n",
        "    print(\"2. That the file is in your main 'My Drive' area and not inside a folder.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "_exTXPWr6qlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaJ_nWdT4mcL"
      },
      "outputs": [],
      "source": [
        "## libraries to install\n",
        "!pip install flask pyngrok insightface==0.7.3 diffusers transformers accelerate onnxruntime-gpu --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First paste your hugging face token in secret(key) section\n",
        "go to hugging face. login go to setting then access token create new token and then go in read then generate token. copy that and paste in secret section in colab.                 \n",
        "name should HF_TOKEN"
      ],
      "metadata": {
        "id": "Sc1lFURx5ryS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File: colab_ai_worker.py\n",
        "# Purpose: Runs in Google Colab to perform heavy AI computation.\n",
        "\n",
        "# --- 2. IMPORTS ---\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from flask import Flask, request, send_file\n",
        "from pyngrok import ngrok, conf\n",
        "import io\n",
        "import logging\n",
        "import json\n",
        "\n",
        "# Suppress pyngrok and werkzeug logging to keep the output clean\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "log = logging.getLogger('werkzeug')\n",
        "log.setLevel(logging.ERROR)\n",
        "\n",
        "# --- 3. SETUP MODEL FILE FROM GOOGLE DRIVE ---\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_file_path = '/content/drive/My Drive/inswapper_128.onnx'\n",
        "target_file_path = 'inswapper_128.onnx'\n",
        "\n",
        "if os.path.exists(drive_file_path):\n",
        "    print(\"Copying 'inswapper_128.onnx' from Google Drive...\")\n",
        "    shutil.copy(drive_file_path, target_file_path)\n",
        "    print(\"✅ Model file is ready.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"CRITICAL ERROR: The file 'inswapper_128.onnx' was not found in your Google Drive at '{drive_file_path}'\")\n",
        "\n",
        "# --- 4. THE AI LOGIC ---\n",
        "class FaceConsistentFashionAI:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.face_app = None\n",
        "        self.fashion_pipeline = None\n",
        "        self.face_swapper = None\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        print(\"Setting up face analysis model...\")\n",
        "        self.face_app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "        self.face_app.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "        print(\"Loading face swapper model...\")\n",
        "        self.face_swapper = insightface.model_zoo.get_model(target_file_path, download=False, download_zip=False)\n",
        "\n",
        "        print(\"Setting up diffusion model...\")\n",
        "        model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
        "        self.fashion_pipeline = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
        "\n",
        "        self.fashion_pipeline.scheduler = DPMSolverMultistepScheduler(\n",
        "            beta_start=0.00085,\n",
        "            beta_end=0.012,\n",
        "            beta_schedule=\"scaled_linear\",\n",
        "            num_train_timesteps=1000,\n",
        "            trained_betas=None,\n",
        "            prediction_type=\"epsilon\",\n",
        "            thresholding=False,\n",
        "            algorithm_type=\"sde-dpmsolver++\",\n",
        "            solver_type=\"midpoint\",\n",
        "            lower_order_final=True,\n",
        "            use_karras_sigmas=True\n",
        "        )\n",
        "\n",
        "        self.fashion_pipeline = self.fashion_pipeline.to(self.device)\n",
        "        print(\"✅ All AI models loaded successfully!\")\n",
        "\n",
        "    def get_source_face(self, image_bytes):\n",
        "        nparr = np.frombuffer(image_bytes, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        faces = self.face_app.get(img)\n",
        "        if not faces: return None\n",
        "        return sorted(faces, key=lambda x: (x.bbox[2]-x.bbox[0])*(x.bbox[3]-x.bbox[1]), reverse=True)[0]\n",
        "\n",
        "    def run_generation(self, source_face, height, weight, age, gender, style, occasion, preferred_colors, clothing_items):\n",
        "        body_description = f\"a {age}-year-old {gender}, approximately {height}cm tall and {weight}kg body type\"\n",
        "        clothing_list_str = \", \".join(clothing_items)\n",
        "\n",
        "        # Shortened prompt to be under the 77-token limit and avoid warnings\n",
        "        prompt = (f\"Full body photo of {body_description}. \"\n",
        "                  f\"Wearing a {style} outfit for {occasion}, including a {clothing_list_str}. \")\n",
        "        if preferred_colors:\n",
        "            prompt += f\"Color palette: {preferred_colors}. \"\n",
        "        prompt += \"Professional studio lighting, high fashion, photorealistic, sharp focus, 8k.\"\n",
        "\n",
        "        negative_prompt = (\"ugly, deformed, disfigured, poor details, bad anatomy, blurry, low quality, \"\n",
        "                           \"distorted, extra limbs, missing limbs, multiple people, watermark, text, logo, \"\n",
        "                           \"cartoon, anime, painting, drawing, cropped head, out of frame.\")\n",
        "        if gender == \"male\": negative_prompt += \" woman, female, girl, feminine features.\"\n",
        "        elif gender == \"female\": negative_prompt += \" man, male, boy, masculine features.\"\n",
        "\n",
        "        print(f\"\\n--- PROMPT ---\\n{prompt}\\n--------------\\n\")\n",
        "        generator = torch.Generator(device=self.device).manual_seed(torch.randint(0, 1000000, (1,)).item())\n",
        "        base_image = self.fashion_pipeline(prompt=prompt, negative_prompt=negative_prompt, num_inference_steps=30,\n",
        "                                           guidance_scale=7.5, width=512, height=768, generator=generator).images[0]\n",
        "\n",
        "        generated_img_cv2 = cv2.cvtColor(np.array(base_image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        target_faces = self.face_app.get(generated_img_cv2)\n",
        "        if target_faces:\n",
        "            target_face = sorted(target_faces, key=lambda x: (x.bbox[2]-x.bbox[0])*(x.bbox[3]-x.bbox[1]), reverse=True)[0]\n",
        "            swapped_image = self.face_swapper.get(generated_img_cv2, target_face, source_face, paste_back=True)\n",
        "            return Image.fromarray(cv2.cvtColor(swapped_image, cv2.COLOR_BGR2RGB))\n",
        "        return base_image\n",
        "\n",
        "# --- 5. FLASK SERVER TO EXPOSE THE AI ---\n",
        "app = Flask(__name__)\n",
        "fashion_ai = FaceConsistentFashionAI()\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def handle_generation():\n",
        "    print(\"\\nAI Worker: Received request...\")\n",
        "    if 'user_image' not in request.files: return \"Missing 'user_image' file\", 400\n",
        "    try:\n",
        "        file = request.files['user_image']\n",
        "        height = request.form.get('height', type=int)\n",
        "        weight = request.form.get('weight', type=int)\n",
        "        age = request.form.get('age', type=int)\n",
        "        gender = request.form.get('gender')\n",
        "        style = request.form.get('style')\n",
        "        occasion = request.form.get('occasion')\n",
        "        preferred_colors = request.form.get('preferred_colors', '')\n",
        "        clothing_items = json.loads(request.form.get('clothing_items'))\n",
        "\n",
        "        source_face = fashion_ai.get_source_face(file.read())\n",
        "        if not source_face: return \"Could not detect a face in the uploaded image.\", 400\n",
        "\n",
        "        print(f\"AI Worker: Generating image for a {gender}...\")\n",
        "        final_image = fashion_ai.run_generation(source_face, height, weight, age, gender, style, occasion, preferred_colors, clothing_items)\n",
        "\n",
        "        buf = io.BytesIO()\n",
        "        final_image.save(buf, format='PNG')\n",
        "        buf.seek(0)\n",
        "\n",
        "        print(\"AI Worker: Sending image back.\")\n",
        "        return send_file(buf, mimetype='image/png')\n",
        "    except Exception as e:\n",
        "        print(f\"AI Worker ERROR: {e}\")\n",
        "        return str(e), 500\n",
        "\n",
        "# --- 6. NGROK TUNNEL TO EXPOSE THE SERVER --\n",
        "-\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "conf.get_default().auth_token = \"your ngrok token\"  # <--- PASTE YOUR NGROK TOKEN HERE\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"================================================================\")\n",
        "print(f\"✅ AI Worker is LIVE at: {public_url}\")\n",
        "print(\"COPY THIS URL and paste it into your fastapi_backend.py script.\")\n",
        "print(\"================================================================\")\n",
        "\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "7_wUtAus47a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after running above code you get a link like this \"https://3431-35-247-122-107.ngrok-free.app\"\n",
        "# paste that link in backend file then run backend after that run frontend"
      ],
      "metadata": {
        "id": "xHyyIZim7PcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}